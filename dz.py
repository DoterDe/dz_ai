# Импортируем необходимые библиотеки
import tensorflow as tf  # TensorFlow для создания и обучения нейронной сети
import numpy as np  # NumPy для работы с массивами и математики

# Генерация чисел Фибоначчи
def fibonacci_sequence(n):
    fib_seq = [0, 1]  # Начальная последовательность Фибоначчи
    for i in range(2, n):  # Начинаем с третьего элемента
        fib_seq.append(fib_seq[-1] + fib_seq[-2])  # Каждый следующий элемент равен сумме двух предыдущих
    return np.array(fib_seq)  # Возвращаем последовательность в виде NumPy массива

# Создание входных данных X и целевых y
n = 100  # Количество чисел в последовательности
fib_seq = fibonacci_sequence(n)  # Генерация последовательности Фибоначчи

# Входные данные (все числа кроме последнего)
X = fib_seq[:-1]  # Берем все элементы, кроме последнего (для обучения)
y = fib_seq[1:]   # Берем все элементы, начиная с второго (для целей обучения, сдвиг на 1)

# Нормализуем данные (делим на максимальное значение, чтобы ускорить обучение)
max_value = np.max(fib_seq)  # Находим максимальное значение в последовательности
X = X / max_value  # Нормализуем входные данные
y = y / max_value  # Нормализуем целевые данные

# Преобразуем данные в нужный формат для подачи в нейронную сеть
X = np.array(X, dtype=np.float32).reshape(-1, 1)  # Преобразуем X в массив float32 и делаем форму (n, 1)
y = np.array(y, dtype=np.float32).reshape(-1, 1)  # То же для y, форма (n, 1)

# Создание модели нейронной сети
model = tf.keras.Sequential([
    # Первый скрытый слой: 16 нейронов, функция активации ReLU
    tf.keras.layers.Dense(16, activation='relu', input_dim=1),  # Входной слой с 1 нейроном (по одному числу на вход)
    
    # Второй скрытый слой: 32 нейрона, функция активации ReLU
    tf.keras.layers.Dense(32, activation='relu'),  # Дополнительный скрытый слой с 32 нейронами
    
    # Выходной слой: 1 нейрон (предсказание следующего числа)
    tf.keras.layers.Dense(1)  # Выходной слой для одного значения (предсказание числа)
])

# Компиляция модели
model.compile(optimizer='adam', loss='mean_squared_error')  # Используем оптимизатор Adam и MSE как функцию потерь

# Обучение модели
model.fit(X, y, epochs=300, verbose=0)  # Обучаем модель на данных X и y, 300 эпох для лучшего обучения

# Пример предсказания (предсказание следующего числа после 100)
# Нормализуем входное значение (последний элемент последовательности)
predicted_value = model.predict(np.array([[fib_seq[-1] / max_value]], dtype=np.float32))  # Вводим последний элемент, нормализованный

# Приводим предсказанное значение обратно к масштабу
predicted_value = predicted_value * max_value  # Ожидаемое следующее число в оригинальном масштабе

# Выводим результат
print(f"Предсказанное следующее число в последовательности: {predicted_value.flatten()[0]}")
